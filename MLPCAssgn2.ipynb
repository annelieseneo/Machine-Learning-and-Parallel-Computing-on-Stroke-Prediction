# -*- coding: utf-8 -*-
"""
Created on Tue Jun  8 21:52:16 2021

@author: User
"""

# import pandas
import pandas as pd

# import numpy for working with numbers
import numpy as np
import math

# import plots
import matplotlib.pyplot as plt

from IPython import get_ipython
get_ipython().run_line_magic('matplotlib','inline')


# DATA OBSERVATION AND UNDERSTANDING

# DATA SOURCE

# import data from Excel .csv sheet
df = pd.read_csv('C:\\Users\\User\\Downloads\\ITS66604-MLPC\\strokedataSet.csv')

# show first 5 records of dataset
df.head()

# DATA TYPES

# determine object type of dataset
type(df)

# determine attribute data types
df.info()

# NOISES

# identify attributes with missing values
df.info()
df.isnull().sum() 

# identify incomplete records 
null_df = df[df.isnull().any(axis=1)]

# identify impossible and extreme values
df_desc = df.describe()

# ERRORS

# identify duplicated records
df[df.duplicated(subset = None, keep = False)]

# identify inconsistencies by displaying all unique values of each attribute
for col in df:
    print(df[col].unique())

# ANOMALIES

# identify outliers
df.boxplot(rot = 0, boxprops = dict(color = 'blue'), 
           return_type = 'axes', figsize = (30, 8))
plt.title("Box Plot of Stroke Data", size = 30) # title of plot and its size
plt.suptitle("")
plt.xlabel("Attributes", size = 25) # x axis label
plt.ylabel("Measurements (units)", size = 25) # y axis label
plt.xticks(size = 25) # size of x ticks
plt.yticks(size = 25) # size of y ticks
plt.show()

# data observation: dependencies
# import searborn library for more variety of data visualisation using 
# fewer syntax and interesting default themes
import seaborn as sns

# compare linear relationships between attributes using correlation 
# coefficient generated using correlation heatmap
sns.heatmap(df.corr(), cmap = 'PuBu', annot = True)
plt.show()

# visualise pairs plot or scatterplot matrix to identify weak 
# class-attribute relationship
g = sns.pairplot(df, hue = 'stroke', palette = 'PuBu')
g = g.map_upper(plt.scatter)
g = g.map_lower(sns.kdeplot, common_norm=False)

# data observation: logical relationships

# plot percentage stacked barchart

# one-hot encoding to split the target class labels of 'stroke'
state = pd.get_dummies(df['stroke'])

# rename column to Negative and Positive for stroke
state.rename(columns={0 : 'Negative', 1: 'Positive'}, inplace=True)

gender = df[['gender','stroke']]
gender = pd.concat([gender,state], axis=1)
gender.drop(['stroke'], axis=1, inplace=True)
gender_data = gender.groupby('gender')[['Negative','Positive']].sum()
gender_data = gender_data.apply(lambda x: x*100/sum(x), axis=1)

marr = df[['married','stroke']]
marr = pd.concat([marr,state], axis=1)
marr.drop(['stroke'], axis=1, inplace=True)
marr_data = marr.groupby('married')[['Negative','Positive']].sum()
marr_data = marr_data.apply(lambda x: x*100/sum(x), axis=1)

res = df[['residence','stroke']]
res = pd.concat([res,state], axis=1)
res.drop(['stroke'], axis=1, inplace=True)
res_data = res.groupby('residence')[['Negative','Positive']].sum()
res_data = res_data.apply(lambda x: x*100/sum(x), axis=1)

smk = df[['smoking','stroke']]
smk = pd.concat([smk,state], axis=1)
smk.drop(['stroke'], axis=1, inplace=True)
smk_data = smk.groupby('smoking')[['Negative','Positive']].sum()
smk_data = smk_data.apply(lambda x: x*100/sum(x), axis=1)

# colour for Negative and Positive
colr=['royalblue','tomato']

# plotting
gender_data.plot(kind='bar', stacked=True, # stacked column chart
                 title='Gender Percentage Stacked Chart', # chart title
                 xlabel='Gender', # x axis label
                 ylabel='Percentage (%)', # y axis label
                 color=colr) # colour of columns
plt.xticks(color = 'black', # colour of x ticks
           rotation = 0, # rotation of label
           horizontalalignment = 'right') # alignment
plt.legend(loc='lower right') # position of legend
marr_data.plot(kind='bar', stacked=True, # stacked column chart
               title='Married Percentage Stacked Chart', # chart title
               xlabel='Married', # x axis label
               ylabel='Percentage (%)', # y axis label
               color=colr) # colour of columns
plt.xticks(color = 'black', # colour of x ticks
           rotation = 0, # rotation of label
           horizontalalignment = 'right') # alignment
plt.legend(loc='lower right') # position of legend
res_data.plot(kind='bar', stacked=True, # stacked column chart
              title='Residence Percentage Stacked Chart', # chart title
              xlabel='Residence', # x axis label
              ylabel='Percentage (%)', # y axis label
              color=colr) # colour of columns
plt.xticks(color = 'black', # colour of x ticks
           rotation = 0, # rotation of label
           horizontalalignment = 'right') # alignment
plt.legend(loc='lower right') # position of legend
smk_data.plot(kind='bar', stacked=True, # stacked column chart
              title='Smoking Percentage Stacked Chart', # chart title
              xlabel='Smoking', # x axis label
              ylabel='Percentage (%)', # y axis label
              color=colr) # colour of columns
plt.xticks(color = 'black', # colour of x ticks
           rotation = 0, # rotation of label
           horizontalalignment = 'right') # alignment
plt.legend(loc='lower right') # position of legend

# data observation: manual observation

colors = ['#3d84bf', '#295981']
cols = ['gender','hypertension','heartDisease','married','residence',
        'smoking']
columns = list(cols)
# barplot of the count for all columns' categories
for i in columns :
    fig, ax = plt.subplots(figsize=(4,4))
    bar = df.groupby(i).size().plot(kind='bar', color=colors, ax=ax)
    plt.xticks(rotation=0)
    fig.suptitle("Count of Categories \"" + i + "\"")

# pie chart of count of target class label 'stroke'
labels = 'Positive','Negative'
# import counter
from collections import Counter
count = Counter(df['stroke'])
sizes = [str(count[1]),str(count[0])] # size of slices
colors = ['tomato','royalblue'] # colours of slices
explode = (0,0)
plt.pie(sizes, explode=explode, labels=labels, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140)
plt.title("Target Class Labels") # title of plot

# summary statistics of the attributes, including measures of central tendency 
# and measures of dispersion
df_desc = df.describe()

# display the number of entries, the number of names of the column attributes,
# the data type and digit placings, and the memory space used
df.info()


# DATA PREPROCESSING

# drop duplicated records
df.drop_duplicates(inplace=True)

# check that no duplication
df[df.duplicated(subset = None, keep = False)]

# reset the indexing of the dataset
df.reset_index(inplace=True)
df.drop(['index'],axis=1, inplace=True)

# data cleaning: round up to smooth inconsistent decimal places
df['age'] = df['age'].apply(np.ceil)

# check min and max values using summary statistics of the attributes,
# including measure of central tendency and measure of dispersion
df_desc = df.describe()

# data cleaning: anomalous outliers smooth using winsorization technique
# replace outlier with maximum or minimum non-outlier

# attributes
fn = ['Glucose','bmi']

for feature in fn:
    
    #compute interquartile range (IQR)
    IQR = df[feature].quantile(0.75) - df[feature].quantile(0.25)
    
    # compute maximum and minimum non-outlier value
    minAllowed = df[feature].quantile(0.25)-1.5*IQR
    maxAllowed = df[feature].quantile(0.75)+1.5*IQR
    
    # replace outlier values
    for i in range (len(df[feature])):
        if df[feature][i] < minAllowed:
            df[feature] = df[feature].replace(df[feature][i], minAllowed)
        elif df[feature][i] > maxAllowed:
            df[feature] = df[feature].replace(df[feature][i], maxAllowed)
        else: continue

# confirmed smoothed outliers using boxplot
df.boxplot(rot = 0, boxprops = dict(color = 'blue'), 
           return_type = 'axes', figsize = (30, 8))
plt.title("Box Plot of Stroke Data", size = 30) # title of plot and its size
plt.suptitle("")
plt.xlabel("Attributes", size = 25) # x axis label
plt.ylabel("Measurements (units)", size = 25) # y axis label
plt.xticks(size = 25) # size of x ticks
plt.yticks(size = 25) # size of y ticks
plt.show()

# summary statistics of the attributes, including measures of central tendency 
# and measures of dispersion
df_desc = df.describe()

# data cleaning: missing values fill in with attribute mode for categorical data
# and attribute mean for numerical data
df['gender'].fillna(df['gender'].mode().iloc[0], inplace = True)
df['hypertension'].fillna(df['hypertension'].mode().iloc[0], inplace = True)
df['heartDisease'].fillna(df['heartDisease'].mode().iloc[0], inplace = True)
df['married'].fillna(df['married'].mode().iloc[0], inplace = True)
df['residence'].fillna(df['residence'].mode().iloc[0], inplace = True)
df['smoking'].fillna(df['smoking'].mode().iloc[0], inplace = True)
df['Glucose'].fillna(df['Glucose'].mean(), inplace = True)
df['bmi'].fillna(df['bmi'].mean(), inplace = True)

# display the number of entries, the number of names of the column attributes,
# the data type and digit placings, and the memory space used
df.info()

# data transformation: Min Max Scaling
from sklearn.preprocessing import MinMaxScaler

# define min-max scaler
scaler = MinMaxScaler(feature_range=(0,1))

# transform numerical data using min-max scaling
dfnum = ['age','bmi','Glucose']
scaled_df = df.copy()
scaled_df[dfnum] = scaler.fit_transform(scaled_df[dfnum])

# transform categorical data into numbers by encoding
from sklearn import preprocessing
encoder = preprocessing.LabelEncoder()

# encoding
encoder.fit(scaled_df['gender'])
scaled_df['gender'] = encoder.transform(scaled_df['gender'])
encoder.fit(scaled_df['married'])
scaled_df['married'] = encoder.transform(scaled_df['married'])
encoder.fit(scaled_df['residence'])
scaled_df['residence'] = encoder.transform(scaled_df['residence'])
encoder.fit(scaled_df['smoking'])
scaled_df['smoking'] = encoder.transform(scaled_df['smoking'])

# data reduction: correlation matrix
# compare linear relationships between attributes using correlation 
# coefficient generated using correlation matrix

# scaled data
sns.heatmap(scaled_df.corr(), cmap = 'PuBu', annot = True, fmt='.2f')
plt.show()

# visualise pairs plot or scatterplot matrix to identify 
# weak class-attribute relationship

# scaled data
g = sns.pairplot(scaled_df, hue = 'stroke', palette = 'PuBu')
g = g.map_upper(plt.scatter)
g = g.map_lower(sns.kdeplot, common_norm=False)


# DATA MODELLING

# import train test split module
from sklearn.model_selection import train_test_split

# import DT algorithm from DT class
from sklearn.tree import DecisionTreeClassifier

# import linear regression model
import statsmodels.api as sm

# split dataset into attributes and labels
X = scaled_df.iloc[:, :-1].values # the attributes
y = scaled_df.iloc[:, 9].values # the labels

# choose appropriate range of training set proportions
t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]

# plot DT based on entropy information gain, best splitter, and minimum 2
# sample leaves
DT = DecisionTreeClassifier(splitter = 'best', criterion = 'entropy', 
                            min_samples_leaf = 2)

# find best training set proportion for the chosen DT model
plt.figure()
for s in t:
    scores = []
    for i in range(1,1000):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s, 
                                                            random_state = 19)
        DT.fit(X_train, y_train) # consider DT scores
        scores.append(DT.score(X_test, y_test))
    plt.plot(s, np.mean(scores), 'bo')
plt.title("Accuracies of Training Set Proportions") # title of plot
plt.xlabel('Training Set Proportion') # x axis label
plt.ylabel('Accuracy'); # y axis label

# choose train test splits from original dataset as 80% train data and 
# 20% test data for highest accuracy
from sklearn.model_selection import train_test_split
X_trainDT, X_testDT, y_trainDT, y_testDT = train_test_split(X, y, test_size=0.2, 
                                                            random_state=19)

# number of records in training set
len(X_trainDT)

# count each outcome in training set
count = Counter(y_trainDT)
print(count.items())

# DT CLASSIFICATION MODEL
# prediction modelling
# using DT classifier based on entropy information gain, best splitter, 
# and minimum 2 sample leaves
classifierDT = DecisionTreeClassifier(splitter = 'best', criterion='entropy', 
                                      min_samples_leaf = 2)
classifierDT.fit(X_trainDT, y_trainDT)

# import library to plot tree
from sklearn import tree

fig = plt.figure(figsize = (100, 70))
# target class labels
cn = ['0','1']
# attribute features names
fn = ['gender','age','hypertension','heartDisease','married','residence',
      'Glucose','bmi','smoking']
# plot DT
DT = tree.plot_tree(classifierDT,
                    feature_names = fn,
                    class_names= cn,
                    filled = True)

# identifies the important features
classifierDT.feature_importances_

# extracted rules
dtrules = tree.export_text(classifierDT, feature_names = fn)
print(dtrules)

# model evaluation
# number of records in test set
len(X_testDT)

# count each outcome in test set
count = Counter(y_testDT)
print(count.items())

# use the chosen DT models to make predictions on test data
y_predDT = classifierDT.predict(X_testDT)

# using confusion matrix
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_testDT, y_predDT))
print(classification_report(y_testDT, y_predDT))

# using accuracy performance metric
from sklearn.metrics import accuracy_score
print("Train Accuracy: ", accuracy_score(y_trainDT, classifierDT.predict(X_trainDT)))
print("Test Accuracy: ", accuracy_score(y_testDT, y_predDT))

# new data record must be within the data ranges to avoid extrapolation
df_desc = df.describe()

# create new record
newdata = {'gender':['F'],
           'age':[38],
           'hypertension':[0],
           'heartDisease':[1],
           'married':['No'],
           'residence':['Rural'],
           'Glucose':[115],
           'bmi':[25],
           'smoking':['smokes']}
# create as a dataframe
newdf = pd.DataFrame(newdata, columns=['gender','age','hypertension',
                                       'heartDisease','married','residence',
                                       'Glucose','bmi','smoking'])

# transformation with min-max scaling
scaled_newdf = newdf.copy()
scaled_newdf[dfnum] = scaler.fit_transform(scaled_newdf[dfnum])

# encoding
encoder.fit(df['gender'])
scaled_newdf['gender'] = encoder.transform(scaled_newdf['gender'])
encoder.fit(df['married'])
scaled_newdf['married'] = encoder.transform(scaled_newdf['married'])
encoder.fit(df['residence'])
scaled_newdf['residence'] = encoder.transform(scaled_newdf['residence'])
encoder.fit(df['smoking'])
scaled_newdf['smoking'] = encoder.transform(scaled_newdf['smoking'])

# compute probabilities of assigning to each of the two classes of stroke
probaDT = classifierDT.predict_proba(scaled_newdf)
probaDT.round(4) # round probabilities to four decimal places, if applicable

# make prediction of class label
predDT = classifierDT.predict(scaled_newdf)
predDT


# MULTIPLE LINEAR REGRESSION MODEL

# find variance of all attributes
scaled_df.var()

df_MLR = scaled_df.copy() # full dataset
y_MLR = df_MLR.pop('Glucose') # target class label
X_MLR = df_MLR # attributes

# train test splits as 80% train set and 20% test set
X_trainMLR, X_testMLR, y_trainMLR, y_testMLR = train_test_split(X_MLR, y_MLR, 
                                                                test_size=0.2, 
                                                                random_state=19)

# building the model
X_train_MLR = sm.add_constant(X_trainMLR) # adds a constant column to input data 
                                        # set to better consider the y intercept
lr_1 = sm.OLS(y_trainMLR, X_train_MLR).fit() # fit train data to model
lr_1.summary() # summary of OLS Regression Results

# import to check the VIF values of the attributes 
from statsmodels.stats.outliers_influence import variance_inflation_factor

# create a dataframe to list all attributes and their VIFs
vif = pd.DataFrame()
vif['Features'] = X_trainMLR.columns # attributes
vif['VIF'] = [variance_inflation_factor(X_trainMLR.values, i) 
              for i in range(X_trainMLR.shape[1])] # find VIFs
vif['VIF'] = round(vif['VIF'], 2) # VIFs round to 2 decimal places
vif = vif.sort_values(by = "VIF", ascending = False) # sort in descending order
print(vif)

# drop the most statistically insignificant attribute with p-value 
# over 0.05 strongly indicating null hypothesis
X = X_trainMLR.drop('smoking', 1)

# rebuild and fit model to new set of attributes
X_train_MLR = sm.add_constant(X) # adds a constant column to input data set to 
                                # better consider the y intercept
lr_2 = sm.OLS(y_trainMLR, X_train_MLR).fit() # fit model

# summary of OLS Regression Results
lr_2.summary()

# create another dataframe to list new set of attributes and their VIFs
vif = pd.DataFrame() # create dataframe
vif['Features'] = X.columns # new set of attributes
vif['VIF'] = [variance_inflation_factor(X.values, i) 
              for i in range(X.shape[1])] # find VIFs
vif['VIF'] = round(vif['VIF'], 2) # round VIFs to 2 decimal places
vif = vif.sort_values(by = "VIF", ascending = False) # sort in descending order
print(vif)

# drop the most statistically insignificant attribute with p-value 
# over 0.05 strongly indicating null hypothesis
X = X_trainMLR.drop(['smoking','married'], 1)

# rebuild and fit model to new set of attributes
X_train_MLR = sm.add_constant(X) # adds a constant column to input data set to 
                                # better consider the y intercept
lr_3 = sm.OLS(y_trainMLR, X_train_MLR).fit() # fit model

# summary of OLS Regression Results
lr_3.summary()

# create another dataframe to list new set of attributes and their VIFs
vif = pd.DataFrame() # create dataframe
vif['Features'] = X.columns # new set of attributes
vif['VIF'] = [variance_inflation_factor(X.values, i) 
              for i in range(X.shape[1])] # find VIFs
vif['VIF'] = round(vif['VIF'], 2) # round VIFs to 2 decimal places
vif = vif.sort_values(by = "VIF", ascending = False) # sort in descending order
print(vif)

# drop the most statistically insignificant attribute with p-value 
# over 0.05 strongly indicating null hypothesis
X = X_trainMLR.drop(['smoking','married','residence'], 1)

# rebuild and fit model to new set of attributes
X_train_MLR = sm.add_constant(X) # adds a constant column to input data set to 
                                # better consider the y intercept
lr_4 = sm.OLS(y_trainMLR, X_train_MLR).fit() # fit model

# summary of OLS Regression Results
lr_4.summary()

# create another dataframe to list new set of attributes and their VIFs
vif = pd.DataFrame() # create dataframe
vif['Features'] = X.columns # new set of attributes
vif['VIF'] = [variance_inflation_factor(X.values, i) 
              for i in range(X.shape[1])] # find VIFs
vif['VIF'] = round(vif['VIF'], 2) # round VIFs to 2 decimal places
vif = vif.sort_values(by = "VIF", ascending = False) # sort in descending order
print(vif)

# residual analysis of train data
y_train_pred = lr_4.predict(X_train_MLR)

# plot histogram of error terms
fig = plt.figure()
sns.distplot((y_trainMLR - y_train_pred), bins = 20) # histogram with 20 bins
fig.suptitle('Error Terms', fontsize = 20) # title of plot 
plt.xlabel('Errors (in units)', fontsize = 18) # x axis label
plt.ylabel('Density', fontsize = 18)
plt.xticks(size = 13) # size of x ticks
plt.yticks(size = 13) # size of y ticks

# predict the test set

# adds a constant column to test set to better consider the y intercept
X_test_m4 = sm.add_constant(X_testMLR)

# drop all attributes that were previously decided as statistically 
# insignificant with p-value over 0.05 strongly indicating null hypothesis
X_test_m4 = X_test_m4.drop(['smoking','married','residence'], axis = 1)

# make predictions using final model
y_pred_m4 = lr_4.predict(X_test_m4)
y_pred_m5 = lr_4.predict(X_train_MLR)

# find R2 score
from sklearn.metrics import r2_score
r2_score(y_true = y_testMLR, y_pred = y_pred_m4)
r2_score(y_true = y_trainMLR, y_pred = y_pred_m5)

# new data record must be within the data ranges to avoid extrapolation
df_desc = df.describe()

# create new record
newdata2 = {'gender':['F','M'],
           'age':[38,64],
           'hypertension':[0,1],
           'heartDisease':[1,1],
           'married':['No','No'],
           'residence':['Rural','Urban'],
           'bmi':[25,23],
           'smoking':['smokes','formerly'],
           'stroke':[1,0]}

# create as dataframe
newdf2 = pd.DataFrame(newdata2, columns=['gender','age','hypertension',
                                       'heartDisease','married','residence',
                                       'bmi','smoking','stroke'])

# min-max scaling
dfnum = ['age','bmi']
scaled_newdf2 = newdf2.copy()
scaled_newdf2[dfnum] = scaler.fit_transform(scaled_newdf2[dfnum])

# adding constant variable to test dataframe
X_test_new = sm.add_constant(scaled_newdf2, has_constant='add')

# encoding
encoder.fit(df['gender'])
X_test_new['gender'] = encoder.transform(X_test_new['gender'])
encoder.fit(df['married'])
X_test_new['married'] = encoder.transform(X_test_new['married'])
encoder.fit(df['residence'])
X_test_new['residence'] = encoder.transform(X_test_new['residence'])
encoder.fit(df['smoking'])
X_test_new['smoking'] = encoder.transform(X_test_new['smoking'])

# create X_test dataframe by dropping feature-selected attributes 
# from the latest X_test_m4
X_test_new = X_test_new.drop(['smoking','married','residence'], axis = 1)

# make predictions of target class label
y_pred_new = lr_4.predict(X_test_new)
y_pred_new

# revert to Glucose values before min-max scaling
b = (df['Glucose'].max()-df['Glucose'].min()) # data range before min-max scaling
y_pred_new[0] = (y_pred_new[0] * b + df['Glucose'].min()) # revert
y_pred_new[1] = (y_pred_new[1] * b + df['Glucose'].min()) # revert
y_pred_new
